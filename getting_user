from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from time import sleep
from selenium.webdriver import ActionChains
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
import time
import os
import datetime
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup as soup
from urllib.request import urlopen as uReq
import requests
import csv

#Input username of the account you want to scrape from here
account_list = ['-------','--------','--------'] # list of accounts you want to scrape

lst = []


page = "followers"  # from following or followers

options = webdriver.ChromeOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--user-agent="Mozilla/5.0 (iPhone; CPU iPhone OS 12_1_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/16D57"')

driver = webdriver.Chrome(ChromeDriverManager().install())


driver.get('https://www.instagram.com')
sleep(2)

username_el = driver.find_element_by_name("username")
username_el.send_keys("------------------") #input your username
time.sleep(0.5)
password_el = driver.find_element_by_name("password")
password_el.send_keys("------------------") #input your password
sleep(0.5)

submit_btn_el = driver.find_element_by_css_selector("button[type='submit']")

submit_btn_el.click()
time.sleep(1)
not_now = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Not Now')]"))).click()
time.sleep(1)
not_now2 = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Not Now')]"))).click()


for i in account_list:
  account = i
  new_user_url = "https://www.instagram.com/" + account +"/"
  driver.get(new_user_url)
  followers = driver.find_elements_by_xpath('//span[@class="g47SY "]')
  followers_num = followers[1].get_attribute("title")
  followers_num = followers_num.replace(',', '')
  follow_value = int(followers_num)

  count = follow_value  # number of profiles you want to get


  driver.find_element_by_xpath('//a[contains(@href, "%s")]' % page).click()
  scr2 = driver.find_element_by_xpath('//*[@id="react-root"]/section/main/div/header/section/ul/li[2]/a')
  sleep(1)
  text1 = scr2.text
  print(text1)
  x = datetime.datetime.now()
  print(x)
  li = []
  sleep(1)
  for i in range(1,count):
    scr1 = driver.find_element_by_xpath('/html/body/div[5]/div/div/div[2]/ul/div/li[%s]' % i)
    driver.execute_script("arguments[0].scrollIntoView();", scr1)
    sleep(0.2)
    text = scr1.text
    list = text.encode('utf-8').split()
    dirname = os.path.dirname(os.path.abspath(__file__))
    csvfilename = os.path.join(dirname, account + "-" + page + ".txt")
    file_exists = os.path.isfile(csvfilename)
    f = open(csvfilename,'a')
    f.write(str(list[0]) + "\r\n")
    li.append(str(list[0]))
    f.close()
    print('{};{}'.format(i, list[0]))
    #print(i + ";" + list[0])
    if i == (count-1):
       print(x)
  li = [e[2:-1] for e in li]


  for i in range(len(li)):
      user_url = "https://www.instagram.com/" + li[i] +"/"
      driver.get(user_url)
      description = driver.find_element_by_xpath("//meta[@property='og:description']")
      followers = description.get_attribute("content")
      follower = followers.split(" ")
      follow_num = follower[0]
      if "k" in follow_num:
        pass
      else:
        number = int(follow_num.replace(',', ''))
        if number < 1000:
          pass
        else:
          lst.append([li[i],follow_num])
    

driver.quit()

with open('follower_list.csv','w',newline='') as fi:
  a = csv.writer(fi, delimiter=',')
  data = lst
  a.writerows(data)
